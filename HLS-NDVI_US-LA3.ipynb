{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc55e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr, ogr\n",
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206cca30",
   "metadata": {},
   "source": [
    "The 1st section below - setting the path to input files. Set \"clipped_files_exist\" to 1 if you want to use previously created clipped files for the classification.\n",
    "\n",
    "There are recommendations in the HLS V2 Manual to complete cloud musking including:cirrus, cloud, adjacent cloud pixels. Add integer numbers to \"i_filter\" list if you want to filter the following:\n",
    "1 - cirrus pixels(bit number 0 (to read binary in python it should have index [-1]);\n",
    "2 - cloud pixels(bit number 1 [-2];\n",
    "3 - adjacent cloud pixels (bit number 2 [-3]);\n",
    "4 - cloud shadow pixels (bit number 3 [-4]);\n",
    "5 - snow/ice pixels (bit number 4 [-5]);\n",
    "6 - water pixels (bit number 5 [-6])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd() # use current directory\n",
    "i_filter = [2] # see comment above. \"Minus\" sign added later\n",
    "#script_path = os.path.join(\"U:\",\"HLS_project\",\"data\")#'U:/HLS_project/data/' # to save files in folders different from the current directory (should contain the same folders as in the current directory)\n",
    "#input_path = script_path # in case .hdf files are in the \"/input/\" folder relative to the current directory\n",
    "input_path = os.path.join(\"/\",\"media\",\"oleks\",\"Transcend\",\"OSU\",\n",
    "                          \"HLSv2_LA3_data\",\"structured_HLS.v2.0.16RBT.2016.2022.LA3.Bands_RED_NIR_QA\",\"16RBT\",\"2018\")# '/media/oleks/Transcend/OSU/OWC_DATA/2016/' # in case of using hdf files outside the current directory\n",
    "QA_in_3band_tif = 0 # 1 - if clipped files are 3-band tif, 0 - if bands are in separate files\n",
    "clip_feature = os.path.join(script_path,\"shape\",\"polygon_LA3_1000m_buffer_32616.shp\") # path to the shape file used for clipping\n",
    "path_Sentinel_data = os.path.join(input_path, \"input\", \"S\")\n",
    "path_Landsat_data = os.path.join(input_path, \"input\", \"L\")\n",
    "print(\"script_path: \", script_path)\n",
    "print(\"input_path: \", input_path)\n",
    "# !!! TO CREATE NEW  CLIPPED FILES SET QA_in_3band_tif = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e98600",
   "metadata": {},
   "source": [
    "HLS v2 from LC DAAC (each band is separate).\n",
    "This section opens .tif files and creates clipped .tif files of the area of interest. Make sure the \"input_path\" directory contains '/input/S/' and '/input/L/' folders with downloaded files. If only Landsat or Sentinal data is used, set \"path_Sentinel_data\" or \"path_Landsat_data\" some empty folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ea15c",
   "metadata": {},
   "source": [
    "!!! For the HLS v2 files from LC DAAC use this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea17fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that script_path directory contains the next folders:\n",
    "folders = [os.path.join(script_path, \"output\", \"interim_files\"),\n",
    "          os.path.join(script_path, \"output\", \"interim_files\",\"vrt\"),\n",
    "          os.path.join(script_path, \"output\", \"interim_files\",\"bands\"),\n",
    "          os.path.join(script_path, \"output\", \"QA_TIF\"),\n",
    "          os.path.join(script_path, \"output\",\"3BANDS_TIF\")]\n",
    "clf = os.path.join(script_path, \"output\", \"QA_TIF\") # folder for separate QA .tif files\n",
    "#to make sure that previous files are copied to different folder:\n",
    "answ = input(\"Do you want to delete all previous output files before the start? \"\\\n",
    "            \"Enter [y] - Yes; [n] - No. Any answer except [y] terminates execution\") \n",
    "if answ == 'y': \n",
    "    for clf in folders:\n",
    "        for filename in os.listdir(clf):\n",
    "                if filename.endswith(\".tif\") or filename.endswith(\".xml\") or filename.endswith(\".vrt\"):\n",
    "                    os.remove(os.path.join(clf, filename))\n",
    "    print(\"Previous files have been deleted\")\n",
    "elif answ == 'n':  \n",
    "    print(\"Execution terminated\")\n",
    "    quit()\n",
    "else:\n",
    "    print(\"Execution terminated\")\n",
    "    quit()\n",
    "# opening bands (in separate .tif files)\n",
    "files_to_open = []\n",
    "clipped_bands = []\n",
    "vrt_path = os.path.join(script_path, \"output\", \"interim_files\",\"vrt\") # .vrt files are used to create 3-band .tif files\n",
    "folder_paths = [path_Sentinel_data, path_Landsat_data]\n",
    "output_folder = os.path.join(script_path, \"output\")\n",
    "for folder_path in folder_paths:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\"Fmask.tif\"): \n",
    "            fp = os.path.join(folder_path, filename)\n",
    "            files_to_open.append(fp)\n",
    "print(str(len(files_to_open)) + \" files detected\")\n",
    "#opening files hdf files in a loop\n",
    "f_count = 0\n",
    "for f in files_to_open:\n",
    "    image_date = f[-29: -22] # part of the filename containing year+day\n",
    "    input_fname = f[-44: -10] # final output name of .tif file\n",
    "    layer_bands = []\n",
    "    red_band = input_fname + \".B04.tif\"\n",
    "    qa_band = input_fname + \".Fmask.tif\"\n",
    "    if \"HLS.L30.\" in f: # if file is from the Landsat (L) dataset\n",
    "        nir_band = input_fname + \".B05.tif\"\n",
    "        db = \".HLS.L30.\"\n",
    "        folder_path = os.path.join(input_path, \"input\", \"L\") \n",
    "    if \"HLS.S30.\" in f: # if file is from the Sentinel (S) dataset\n",
    "        nir_band = input_fname + \".B8A.tif\"\n",
    "        db = \".HLS.S30.\"\n",
    "        folder_path = os.path.join(input_path, \"input\", \"S\")\n",
    "    bands = [red_band,nir_band,qa_band]\n",
    "    in_vrt = [] \n",
    "    for b in bands:\n",
    "        if \"B05\" in b or \"8A\" in b:\n",
    "            suffix = \"NIR\"\n",
    "        if \"B04\" in b:\n",
    "            suffix = \"RED\"\n",
    "        if \"Fmask\" in b:\n",
    "            suffix = \"_QA\"\n",
    "        layer_band = image_date + db + \"band\" + suffix # \n",
    "        layer_bands.append(layer_band)\n",
    "        in_data = os.path.join(folder_path, b)\n",
    "        sd = gdal.Open(in_data)\n",
    "        arr = sd.ReadAsArray()\n",
    "        [cols, rows] = arr.shape\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        in_raster = os.path.join(script_path, \"output\", \"interim_files\", in_data + \".tif\")\n",
    "        if \"Fmask\" in b: \n",
    "            outdata = driver.Create('in_raster.tif', rows, cols, 1, gdal.GDT_Int16)\n",
    "        else:\n",
    "            outdata = driver.Create('in_raster.tif', rows, cols, 1, gdal.GDT_Float32) #gdal.GDT_UInt16\n",
    "        outdata.SetGeoTransform(sd.GetGeoTransform())##sets same geotransform as input\n",
    "        outdata.SetProjection(sd.GetProjection())##sets same projection as input\n",
    "        outdata.GetRasterBand(1).WriteArray(arr)\n",
    "        outdata.GetRasterBand(1).SetNoDataValue(-9999)## no data value should be -9999\n",
    "        outdata.FlushCache() ##saves to disk\n",
    "        outdata = None\n",
    "        sd = None\n",
    "        data = None\n",
    "        \n",
    "        out_raster = os.path.join(script_path, \"output\", \"interim_files\", \"bands\", layer_band +\".tif\")\n",
    "        in_vrt.append(out_raster) # creating a list for the next step - building virtual 3-band raster \n",
    "        gdal.Warp(out_raster, 'in_raster.tif', cutlineDSName=clip_feature, cropToCutline=True)\n",
    "        os.remove('in_raster.tif')\n",
    "\n",
    "    # for building virtual 3-band raster:\n",
    "    ImageList = [in_vrt[0], in_vrt[1], in_vrt[2]]\n",
    "    VRT = os.path.join(vrt_path, image_date + \".vrt\")\n",
    "\n",
    "    # !!! osgeo.gdal fails to save bands in different formats\n",
    "\n",
    "    gdal.BuildVRT(VRT, ImageList, separate=True, callback=gdal.TermProgress_nocb)\n",
    "    InputImage = gdal.Open(VRT, 0)  # open the VRT in read-only mode\n",
    "    \n",
    "    output_name = image_date + \".\" + input_fname[-44: -20] + \".v2.0.tif\" # final output name of .tif file\n",
    "    TRANSL = os.path.join(script_path, \"output\", \"3BANDS_TIF\", output_name)\n",
    "    gdal.Translate(TRANSL, InputImage, format='GTiff', creationOptions=['COMPRESS:DEFLATE', 'TILED:YES'],\\\n",
    "               callback=gdal.TermProgress_nocb)\n",
    "    del InputImage  # close the VRT        \n",
    "    f_count += 1\n",
    "    print(\"completed: \", f_count, \" of \", len(files_to_open), \" files\")\n",
    "    ds = None\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec634ca",
   "metadata": {},
   "source": [
    "This section helps to find some info about files: width, height and number of pixels belonging to the wetland. Check clipped files, find one that is not spoiled. Run the code and enter the year (4 digits) and doy of this file (3-digit, day of the year) without using space key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete temporary files:\n",
    "\n",
    "folders = [os.path.join(script_path, \"output\", \"interim_files\",\"vrt\"),\n",
    "          os.path.join(script_path, \"output\", \"interim_files\",\"bands\"),\n",
    "          os.path.join(script_path, \"output\", \"interim_files\")]\n",
    "for clf in folders:\n",
    "    for filename in os.listdir(clf):\n",
    "            if filename.endswith(\".tif\") or filename.endswith(\".xml\") or filename.endswith(\".vrt\") \\\n",
    "                                                                        or filename.endswith(\".hdr\"):\n",
    "                if \"QA.tif\" in filename:\n",
    "                    shutil.copy2(os.path.join(clf,filename), os.path.join(script_path, \"output\", \"QA_TIF\", filename))\n",
    "                                                        # copy all QA tif in one folder QA_TIF\n",
    "                os.remove(os.path.join(clf,filename))    \n",
    "year_doy = input(\"Find one file that contains high quality data (check the size of the file). Enter year and doy. Example: 2021001 \")\n",
    "\n",
    "\n",
    "if QA_in_3band_tif == 0:\n",
    "    path_clipped_files = os.path.join(script_path, \"output\", \"QA_TIF\")\n",
    "    try:\n",
    "        src = rasterio.open(os.path.join(path_clipped_files, year_doy + \".HLS.L30.band_QA.tif\"), \"r\")\n",
    "        suf_ds = \".L30.\"\n",
    "    except:\n",
    "        src = rasterio.open(os.path.join(path_clipped_files, year_doy + \".HLS.S30.band_QA.tif\"), \"r\")\n",
    "        suf_ds = \".S30.\"\n",
    "else:\n",
    "    path_clipped_files = os.path.join(script_path, \"output\", \"3BANDS_TIF\")\n",
    "    try:\n",
    "        src = rasterio.open(os.path.join(path_clipped_files, year_doy + '.HLS.L30.T16RBT.v2.0.tif'), \"r\")\n",
    "    except:\n",
    "        src = rasterio.open(os.path.join(path_clipped_files, year_doy + '.HLS.S30.T16RBT.v2.0.tif'), \"r\")\n",
    "raster_width=src.width\n",
    "raster_height=src.height\n",
    "print(\"File dimension is: \", raster_height, \"x\", raster_width, \"(height x width) \")\n",
    "band1 = src.read(1)\n",
    "clipped_area = (band1 > 0).sum()\n",
    "print(\"clipped area in pixels: \", clipped_area)\n",
    "tif_boundaries = src.bounds\n",
    "left_bound = tif_boundaries[0]\n",
    "bottom_bound = tif_boundaries[1]\n",
    "right_bound = tif_boundaries[2]\n",
    "top_bound = tif_boundaries[3]\n",
    "crs_used = src.crs\n",
    "print(tif_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ed25f",
   "metadata": {},
   "source": [
    "These codes calculate and build NDVI time series of each pixel using HLS (Harmonized Landsat and Sentinel-2) dataset near the US-LA3 flus tower. The NDVI time series are then used to classify landcovers. The whole year data of HLS was clipped to the 2-km side extent, and tree bands (red, near infrared and QA) were extracted. In 2018, there were three main land-cover types: 1) open water, 2) Spatina, 3) Juncus.\n",
    "Calculalte ndvi and filter clouds. The data used here only has three bands extracted from the original HLS data. Band 1 is red, band 2 is NIR and band 3 is QA band.\n",
    "*Amended version. If variable \"three_band_tif\" is set to 0, then bands from separate files are used (recommended for new files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bcd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = []\n",
    "fname = []\n",
    "directory = os.path.join(script_path, \"output\", \"3BANDS_TIF\") # the path to the clipped files \n",
    "directory_QA = os.path.join(script_path, \"output\", \"QA_TIF\") # the path to the QA band\n",
    "cloud_temp = np.empty([raster_height,raster_width], dtype = int) # dimension is taken from 'clipping' part of the code\n",
    "cloud_code = []\n",
    "nan = np.nan\n",
    "NaN = np.nan\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        fp = os.path.join(directory, filename)\n",
    "        year_doy = fp[-31:-25]\n",
    "        suf_ds = fp[-25:-15]\n",
    "        qafilename = year_doy + suf_ds + \"band_QA.tif\"\n",
    "        qafp = os.path.join(directory_QA, qafilename)\n",
    "        raster_temp = rasterio.open(fp)\n",
    "        red_temp = raster_temp.read(1) \n",
    "        nir_temp = raster_temp.read(2) #scale factor = 0.0001 for both near and red (in L and S)\n",
    "        qa_temp = rasterio.open(qafp) \n",
    "        QA = qa_temp.read(1)  \n",
    "        if np.all(QA == -9999):\n",
    "            continue\n",
    "        else:  \n",
    "            for m in range (0,raster_height): \n",
    "                for n in range (0,raster_width):\n",
    "                    if QA[m][n] == -9999:\n",
    "                        cloud_temp[m][n] = -9999\n",
    "                    else:\n",
    "                        cloud_temp[m][n] = int((\"{0:08b}\".format(QA[m][n]))[-2])\n",
    "            cloud_copy = copy.copy(cloud_temp)          \n",
    "            cloud_code.append(cloud_copy)\n",
    "            cloud_index = np.argwhere(cloud_copy == 1)\n",
    "            ndvi_temp = np.empty(QA.shape, dtype=rasterio.float32)    \n",
    "            check = np.logical_and(red_temp > 0, nir_temp > 0)\n",
    "            ndvi_temp = np.where(check,(nir_temp - red_temp ) / ( nir_temp + red_temp ),np.nan)\n",
    "            ndvi_temp[cloud_index[:,0],cloud_index[:,1]] = np.nan\n",
    "            ndvi.append(ndvi_temp)\n",
    "            name_temp = filename[4:7]\n",
    "            name_copy = copy.copy(name_temp)\n",
    "            fname.append(name_copy)\n",
    "    else:\n",
    "        continue\n",
    "#get day of year\n",
    "ndvi_doy = []\n",
    "for fname in fname:\n",
    "    ndvi_doy.append(int(fname))\n",
    "print(\"ndvi_doy: \", ndvi_doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0569",
   "metadata": {},
   "source": [
    "Keep only growing season (the whole year for LA3) NDVI images for classification + sorting doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc41603",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_first_date = 0\n",
    "gs_last_date = 367\n",
    "\n",
    "# set nan if there is no need to add first_dgs or last_dgs \n",
    "first_dgs = nan # type doy (if there is not spoiled image close to the first day of growing season)\n",
    "last_dgs = nan # type doy (add doy close to the last day of growing season if needed)\n",
    "ndays = len(ndvi_doy)\n",
    "nrows = raster_height \n",
    "ncols = raster_width \n",
    "pixels_N = raster_height * raster_width \n",
    "ndvi_pixel = np.empty([pixels_N,ndays]) \n",
    "for m in range (0,nrows):\n",
    "    for n in range (0,ncols):        \n",
    "        for i in range(0,ndays):\n",
    "            pixel_row = m*raster_width+n\n",
    "            ndvi_pixel[pixel_row][i] = ndvi[i][m][n]\n",
    "ndvi_pixel1=np.vstack((np.array(ndvi_doy),np.array(ndvi_pixel)))\n",
    "ndvi_pixel2=sorted(np.transpose(ndvi_pixel1),key=lambda x:x[0])\n",
    "ndvi_pixel3=[]\n",
    "n_pixels_wi_nan=len(ndvi_pixel2)\n",
    "grow_s_doy = []\n",
    "for i in range(0,n_pixels_wi_nan):\n",
    "    if ndvi_pixel2[i][0]> gs_first_date and ndvi_pixel2[i][0]< gs_last_date:\n",
    "        ndvi_pixel3.append(ndvi_pixel2[i][:])\n",
    "        grow_s_doy.append(int(ndvi_pixel2[i][0]))\n",
    "ndvi_sorted=np.transpose(np.array(ndvi_pixel3))[1::]\n",
    "ndvi_doy_sorted=np.transpose(np.array(ndvi_pixel3))[0:1]\n",
    "\n",
    "# exclude nans and put each image as a column, each row is ndvi time series of one pixel            \n",
    "nan_index = []\n",
    "ndvi_index = []\n",
    "ndays_growing=np.size(ndvi_doy_sorted,1)\n",
    "ndvi_owc = np.empty([clipped_area,ndays_growing])\n",
    "count = 0\n",
    "for i in range (1, pixels_N):\n",
    "    if np.all(np.isnan(ndvi_sorted[i][:])):\n",
    "        nan_index.append(i)\n",
    "    else:\n",
    "        ndvi_owc[count] = ndvi_sorted[i][:]\n",
    "        ndvi_index.append(i)\n",
    "        count = count+1\n",
    "        continue\n",
    "print(\"growing season doy selected: \", grow_s_doy)\n",
    "if np.isnan(first_dgs) == False:\n",
    "    grow_s_doy.insert(0,first_dgs)\n",
    "if np.isnan(last_dgs) == False:\n",
    "    grow_s_doy.append(last_dgs)\n",
    "#print(\"growing season doy selected (with additional doy(s)): \", grow_s_doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42318a87",
   "metadata": {},
   "source": [
    "Plot all the ndvi images for groving season only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb531e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cmap = matplotlib.cm.get_cmap('RdYlGn')\n",
    "my_cmap.set_under('w')\n",
    "fig = plt.figure()\n",
    "dict_i_doy = {}\n",
    "ind = np.arange(0, len(ndvi_doy), 1).tolist()\n",
    "sorted_ndvi_doy = sorted(ndvi_doy)\n",
    "for i in range (0, len(ind)):\n",
    "    dict_i_doy[ind[i]] = sorted_ndvi_doy [i]\n",
    "dict_grow_s_doy ={}\n",
    "for i in range (0,len(grow_s_doy)):\n",
    "    for j in range (0, len(ind)):\n",
    "        if dict_i_doy[ind[j]] == grow_s_doy[i]:\n",
    "            dict_grow_s_doy[grow_s_doy[i]] = ind[j]\n",
    "for i in range (0, len(ind)):\n",
    "    if np.all(np.isnan(ndvi[i])):\n",
    "        print(\"spoiled: \", ndvi_doy[i])\n",
    "        continue\n",
    "    else:\n",
    "        if dict_i_doy[i] > 0 and dict_i_doy[i] < 367:\n",
    "            plt.figure(i)\n",
    "            plt.imshow(ndvi[i], my_cmap, vmin = -1, vmax = 1)\n",
    "            plt.colorbar()\n",
    "            plt.text(22,5,'DOY:' + str(dict_i_doy[i]), fontsize=10)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            #plt.savefig(str(i))\n",
    "        else:\n",
    "            continue\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb9927",
   "metadata": {},
   "source": [
    "The section below provides custom removal of spoiled DOY's. If there is no need to exclude some days, just run it with empty list excl_doy_list = [ ] \n",
    "Otherwise, in order to exclude spoiled data, add appropriate doy to the list: for example, excl_doy_list = [131, 145, 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a02d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_doy_list = []\n",
    "excl_ind_list = []\n",
    "dict_grow_s_doy = {}\n",
    "dict_updated_grow_s_doy = {}\n",
    "j=0\n",
    "for i in range(0, len(grow_s_doy)):\n",
    "    dict_grow_s_doy[grow_s_doy[i]] = i\n",
    "for i in range(0, len(grow_s_doy)):\n",
    "    if grow_s_doy[i] in excl_doy_list:\n",
    "        excl_ind_list.append(dict_grow_s_doy[grow_s_doy[i]])\n",
    "    else:\n",
    "        dict_updated_grow_s_doy[j] = grow_s_doy[i]\n",
    "        j+=1\n",
    "ndvi_owc_update = ndvi_owc\n",
    "ndvi_owc_update = np.delete(ndvi_owc_update,excl_ind_list,1)\n",
    "print(\"spoiled and suspicious doy data have been excluded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c7243",
   "metadata": {},
   "source": [
    "Interpolate each pixel's NDVI time series using linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75856bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pixels=len(ndvi_owc_update)\n",
    "#ndvi_owc_update = ndvi_owc\n",
    "ndays_growing = (ndvi_owc_update.shape)[1] # in case of deleted days\n",
    "file_index = []\n",
    "#interpolation\n",
    "ndvi_interp = np.empty([n_pixels,ndays_growing])\n",
    "for i in range(0,n_pixels):\n",
    "    ndvi_temp = pd.DataFrame(ndvi_owc_update[i,:])\n",
    "    ndvi_interp_temp = ndvi_temp.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    ndvi_interp[i,:] = np.transpose(ndvi_interp_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77c083",
   "metadata": {},
   "source": [
    "drawing random time series to assess presence of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(dict_updated_grow_s_doy.values())\n",
    "print(x)\n",
    "for part in range(0,5):\n",
    "    fig, ax = plt.subplots(figsize=(13, 5))\n",
    "    for sts in range(0,400):\n",
    "        y = list(ndvi_interp[part*500 + sts])\n",
    "        ax.scatter(x, y, marker = '.',color = 'black')\n",
    "        ax.plot(x, y)\n",
    "    plt.xlabel('doy')\n",
    "    plt.ylabel('ndvi index')\n",
    "    major_tick, minor_tick = [], []\n",
    "    for mjt in range (0,360,30): major_tick.append(mjt)\n",
    "    for mnt in range (0,360,10): major_tick.append(mnt)\n",
    "    ax.set_xticks(major_tick) # Set major Grid\n",
    "    ax.set_xticks(minor_tick, minor=True)# Set minor Grid\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"time series for\", part*400 + sts, \" pixels have been pictured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75998c2",
   "metadata": {},
   "source": [
    "!!!\n",
    "Skip the part below in case of calculating another year except the base year used for obtaining standard series. This part of codes creates shape file containing grid numbers of the area of interest (to overlay upon the Worldveiw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"!!!! the next section rewrites standard series!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiona import collection\n",
    "from shapely.geometry import Point, mapping\n",
    "import csv\n",
    "input_crs = crs_used \n",
    "output_crs = crs_used #rasterio.crs.CRS.from_epsg(32616)\n",
    "current_row = top_bound  \n",
    "current_col = left_bound\n",
    "shape_file_path = os.path.join(script_path, \"output\", \"grid_index.csv\")\n",
    "header = [\"Longitude\", \"Latitude\", \"Grid_number\"]\n",
    "data = []\n",
    "grid_number = 0\n",
    "for r in range (0,raster_height):\n",
    "    for c in range (0, raster_width):\n",
    "        grid_number = r*raster_width + c\n",
    "        current_col += 30\n",
    "        row = [str(current_col), str(current_row), str(grid_number)]\n",
    "        \n",
    "        data.append(row)\n",
    "    current_row -= 30\n",
    "    current_col = left_bound\n",
    "with open(shape_file_path, 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)\n",
    "from csv import DictReader\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "shape_file_path_new = os.path.join(script_path, \"output\", \"grid_index.shp\")\n",
    "# Open a collection for writing.\n",
    "with fiona.open(\n",
    "        shape_file_path_new, 'w', \n",
    "        crs=crs_used, \n",
    "        driver='ESRI Shapefile', \n",
    "        schema={\n",
    "            'geometry': 'Point', \n",
    "            'properties': {\n",
    "                'Grid_number': 'float',\n",
    "                'Longitude': 'float',\n",
    "                'Latitude': 'float', \n",
    "                }}\n",
    "        ) as output:\n",
    "    \n",
    "    # Make a mapping of property names to their Python types. This is used to\n",
    "    # convert text values from the input to proper Python values.\n",
    "    ptypes = {k: fiona.prop_type(v) for k, v in output.schema['properties'].items()}\n",
    "    \n",
    "    def feature(row):\n",
    "        \"\"\"Converts a table row and its index to a Fiona feature dict.\"\"\"\n",
    "        geom = {\n",
    "            'type': 'Point', \n",
    "            'coordinates': [float(row['Longitude']), float(row['Latitude'])] }\n",
    "        props = {k: ptypes[k](v) for k, v in row.items()}\n",
    "        return {\n",
    "            'type': 'Feature',\n",
    "            'geometry': geom, \n",
    "            'properties': props }\n",
    "    \n",
    "    # Read rows and write all feature records in one go, using the feature()\n",
    "    # function defined above.\n",
    "    reader = DictReader(open(shape_file_path), delimiter=',')\n",
    "    output.writerecords(map(feature, reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29fad6",
   "metadata": {},
   "source": [
    "!!!Skip the part below in case of calculating another year except the basÐµ year used for obtaining standard series. This part contains procedure of obtaining grid points numbers from csv file for time series avereging (in order to get standard time series for each patch type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deadbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input manually information regarding visually classified grids to the file '/input/worldview_points.csv'\n",
    "csv_pt = script_path + '/input/worldview_points.csv'\n",
    "nan = np.nan\n",
    "df = pd.read_csv(csv_pt)\n",
    "csv_ptl = df['patch_type']\n",
    "pt_row = df['lat_point']\n",
    "pt_col = df['lon_point']\n",
    "pt_point_number = df['point_number']\n",
    "patch_types_list = []\n",
    "colors = [\"b\",\"r\",\"y\",\"g\"] # \"r\", \"b\", \"o\"  ->  add colors in case more patch types revealed\n",
    "for patch in csv_ptl: # gather unique names of patch types\n",
    "    if patch in patch_types_list:\n",
    "        continue\n",
    "    else:\n",
    "        patch_types_list.append(patch)\n",
    "pt_selected = {}\n",
    "grid_number = []\n",
    "for pt in range(0,len(patch_types_list)):\n",
    "    for i in range(0,len(csv_ptl)):\n",
    "        if csv_ptl[i] == patch_types_list[pt]:\n",
    "            #grid_number.append(int(pt_row[i]*raster_width + pt_col[i]))\n",
    "            grid_number.append(int(pt_point_number[i]))       \n",
    "    pt_selected[patch_types_list[pt]] = grid_number\n",
    "    grid_number = []\n",
    "print(\"patch types & grid indexes: \", pt_selected)\n",
    "doy_366 = np.linspace(0,366,367)\n",
    "for pt in range(0,len(patch_types_list)):\n",
    "    pt_ind = (pt_selected[patch_types_list[pt]])\n",
    "    pt_array = []\n",
    "    i_col = 0\n",
    "    for ind in pt_ind:\n",
    "        patch__interp_st = pd.Series(ndvi_interp[ind]).interpolate(method='linear', limit_direction='both', axis=0)\n",
    "        patch__interp_st366 = np.interp(doy_366, x, patch__interp_st)\n",
    "        #patch__interp_st366 = patch__interp_st\n",
    "        pt_array.append(patch__interp_st366)\n",
    "    pt_mean = np.nanmean(pt_array, axis = 0)\n",
    "    plt.plot(pt_mean,label = patch_types_list[pt])\n",
    "    arr_pt_mean = pd.DataFrame(pt_mean).interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    arr_pt_mean.to_pickle(os.path.join(script_path,\"pkl\", patch_types_list[pt] + \".pkl\"))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('DOY')\n",
    "plt.ylabel('NDVI')\n",
    "#plt.xlim([120,300])\n",
    "#plt.ylim([-0.2,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf1174",
   "metadata": {},
   "source": [
    "!!! the last optional part above!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4038303",
   "metadata": {},
   "source": [
    "Start from here to load the standards and use them to classify other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ab4c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doy_366 = np.linspace(0,366,367)\n",
    "ndvi_interp_366 = np.empty([ndvi_interp.shape[0],367])\n",
    "for i in range (len(ndvi_interp)):\n",
    "    ndvi_interp_366[i] = np.interp(doy_366, x, ndvi_interp[i])\n",
    "fold = os.path.join(script_path, \"pkl\")\n",
    "pikles = []\n",
    "pkl_names = []\n",
    "for filename in os.listdir(fold):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        read_pkl = pd.read_pickle(os.path.join(script_path, \"pkl\",filename))  \n",
    "        pikles.append(read_pkl)\n",
    "        pkl_names.append(filename[:-4])\n",
    "print(pkl_names)\n",
    "for p in pikles:\n",
    "    df = pd.DataFrame(p)\n",
    "    plt.plot(p,linewidth = 4)\n",
    "    for i in range (0,400):\n",
    "        rand = random.randint(0, 4000)\n",
    "        plt.plot(ndvi_interp_366[rand], color='grey',linewidth = 0.1)\n",
    "    unpickled_df = None\n",
    "print(\"length of random ndvi_interp\",len(ndvi_interp_366[rand]))\n",
    "print(\"length of read_pickle\",len(p))\n",
    "lcms = math.lcm(len(ndvi_interp_366[rand]),len(p))\n",
    "plt.show()\n",
    "optimal_bias=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4c720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculating bias between standard series and time series of LA3 during separate year\n",
    "#Juncus + Spartina !!!\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#ts1 = pikles[0].values # open water\n",
    "ts2 = pikles[1].values # juncus\n",
    "ts3 = pikles[2].values # spartina\n",
    "LA3_ts = ndvi_interp_366\n",
    "\n",
    "def mse(bias, ts2, ts3, LA3_ts):\n",
    "    for i in range(len(LA3_ts)):\n",
    "        predicted = ts2 + bias, ts3 + bias\n",
    "        error = np.mean((LA3_ts[i] - np.vstack(predicted)) ** 2)\n",
    "        if error > 0.25:\n",
    "            continue\n",
    "        else:\n",
    "            return error\n",
    "\n",
    "# Use optimization to find the optimal biases\n",
    "initial_bias = 0.0  # Initial guess for the biases\n",
    "result = minimize(mse, initial_bias, args=(ts2, ts3, LA3_ts))\n",
    "\n",
    "# Optimal biases are in result.x\n",
    "optimal_bias = result.x\n",
    "print(result)\n",
    "print(\"Optimal Bias:\", optimal_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating bias between standard series and time series of LA3 during separate year\n",
    "#Spartina only!!!\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#ts1 = pikles[0].values # open water\n",
    "ts2 = pikles[1].values # juncus\n",
    "ts3 = pikles[2].values # spartina\n",
    "LA3_ts = ndvi_interp_366\n",
    "\n",
    "def mse(bias, ts2, ts3, LA3_ts):\n",
    "    for i in range(len(LA3_ts)):\n",
    "        predicted = ts3 + bias\n",
    "        error = np.mean((LA3_ts[i] - np.vstack(predicted)) ** 2)\n",
    "        if error > 0.1:\n",
    "            continue\n",
    "        else:\n",
    "            return error\n",
    "\n",
    "# Use optimization to find the optimal biases\n",
    "initial_bias = 0.0  # Initial guess for the biases\n",
    "result = minimize(mse, initial_bias, args=(ts2, ts3, LA3_ts))\n",
    "\n",
    "# Optimal biases are in result.x\n",
    "optimal_bias = result.x\n",
    "print(result)\n",
    "print(\"Optimal Bias:\", optimal_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c748ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doy_366 = np.linspace(0,366,367)\n",
    "ndvi_interp_366 = np.empty([ndvi_interp.shape[0],367])\n",
    "for i in range (len(ndvi_interp)):\n",
    "    ndvi_interp_366[i] = np.interp(doy_366, x, ndvi_interp[i])\n",
    "fold = os.path.join(script_path, \"pkl\")\n",
    "pikles = []\n",
    "pkl_names = []\n",
    "for filename in os.listdir(fold):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        read_pkl = pd.read_pickle(os.path.join(script_path, \"pkl\",filename))  \n",
    "        pikles.append(read_pkl)\n",
    "        pkl_names.append(filename[:-4])\n",
    "print(pkl_names)\n",
    "for p in pikles:\n",
    "    df = pd.DataFrame(p)\n",
    "    plt.plot(p+optimal_bias,linewidth = 4)\n",
    "    for i in range (0,400):\n",
    "        rand = random.randint(0, 4000)\n",
    "        plt.plot(ndvi_interp_366[rand], color='grey',linewidth = 0.1)\n",
    "    unpickled_df = None\n",
    "print(\"length of random ndvi_interp\",len(ndvi_interp_366[rand]))\n",
    "print(\"length of read_pickle\",len(p))\n",
    "lcms = math.lcm(len(ndvi_interp_366[rand]),len(p))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06513dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define dynamic time warping to measure the similarity between standards and each pixel's ndvi time series\n",
    "def DTWDistance(s1, s2):\n",
    "    DTW={}\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        DTW[(i, -1)] = float('inf')\n",
    "    for i in range(len(s2)):\n",
    "        DTW[(-1, i)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "#calculated the distance by DTW\n",
    "d1 = np.zeros((clipped_area,len(pikles)))   \n",
    "ClassNum = np.zeros((clipped_area,1)) \n",
    "for pt in range(len(pikles)):\n",
    "    pt_list = pikles[pt].values.tolist()\n",
    "    for i in range(0,clipped_area): \n",
    "        data_f = pikles[pt]+optimal_bias\n",
    "        d1[i][pt] = DTWDistance(data_f[0],ndvi_interp[i][:])\n",
    "\n",
    "#assign the class that has the smallest distance as the dominant class of the pixel\n",
    "for i in range(0,clipped_area):\n",
    "     ClassNum[i] = np.argmin(d1[i][:])\n",
    "C = ClassNum+1\n",
    "\n",
    "#add all the nan pixels\n",
    "Class1D = np.empty((pixels_N,1)) \n",
    "Class1D[:] = np.nan\n",
    "len_ndvi = len(ndvi_index) \n",
    "for i in range(0,len_ndvi): \n",
    "    rowNum = ndvi_index[i]\n",
    "    Class1D[rowNum,:] = C[i]\n",
    "\n",
    "#### convert 1D array to 2D\n",
    "Class2D = np.empty([raster_height,raster_width])\n",
    "Class2D[:] = np.nan\n",
    "for i in range (0,pixels_N):\n",
    "    row = np.floor(i/raster_width).astype(int)\n",
    "    column = np.remainder(i,raster_width)\n",
    "    Class2D[row][column] = Class1D[i]\n",
    "\n",
    "#plot the classificaiton result    \n",
    "plt.clf()    \n",
    "from matplotlib import colors    \n",
    "cmap = colors.ListedColormap(['blue','green','brown'])  \n",
    "plt.figure()  \n",
    "plt.imshow(Class2D, cmap)\n",
    "cbar = plt.colorbar(ticks = [1,2,3])\n",
    "cbar.set_ticklabels(pkl_names) \n",
    "#plt.savefig('class2017') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002eec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the classification result to tiff\n",
    "file = script_path + '/projection_files/LA3-template.tif' #using this image's projection information to save the tiff\n",
    "ds = gdal.Open(file)\n",
    "band = ds.GetRasterBand(1)\n",
    "arr = band.ReadAsArray()\n",
    "[cols, rows] = arr.shape\n",
    "driver = gdal.GetDriverByName(\"GTiff\")\n",
    "outdata = driver.Create(\"LA3-HLS-L30-class-2017-v5.3_bias0.06316407.tif\", rows, cols, 1, gdal.GDT_UInt16)\n",
    "outdata.SetGeoTransform(ds.GetGeoTransform())##sets same geotransform as input\n",
    "outdata.SetProjection(ds.GetProjection())##sets same projection as input\n",
    "outdata.GetRasterBand(1).WriteArray(Class2D)\n",
    "outdata.GetRasterBand(1).SetNoDataValue(-9999)##if you want these values transparent\n",
    "outdata.FlushCache() ##saves to disk!!\n",
    "outdata = None\n",
    "band=None\n",
    "ds=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e1124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
